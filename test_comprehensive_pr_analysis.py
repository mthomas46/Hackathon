#!/usr/bin/env python3
"""
COMPREHENSIVE END-TO-END PR CONFIDENCE ANALYSIS TEST

This test demonstrates the complete ecosystem working together:
- Document Store: Stores and retrieves all documents
- Prompt Store: Manages analysis prompts with versioning
- Analysis Service: Performs various analysis types
- Orchestrator: Coordinates the entire workflow
- LLM Gateway: Provides Ollama integration for real AI analysis

Tracks all documents, prompts, and their relationships throughout the analysis.
"""

import asyncio
import json
import httpx
import time
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, List
from dataclasses import dataclass, field

# Ollama configuration
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "llama3.2:latest"

@dataclass
class DocumentArtifact:
    """Tracks a document and its metadata throughout the analysis."""
    id: str
    title: str
    content: str
    source: str
    type: str  # 'pr', 'jira', 'confluence', 'analysis_result', 'report'
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    version: str = "1.0"
    related_documents: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class PromptArtifact:
    """Tracks a prompt and its usage throughout the analysis."""
    id: str
    name: str
    content: str
    category: str  # 'requirements_analysis', 'consistency_check', 'confidence_scoring'
    version: str = "1.0"
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    usage_count: int = 0
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    related_analyses: List[str] = field(default_factory=list)

@dataclass
class AnalysisWorkflow:
    """Tracks the complete analysis workflow and all artifacts."""
    workflow_id: str
    start_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    end_time: datetime = None
    documents_used: List[DocumentArtifact] = field(default_factory=list)
    prompts_used: List[PromptArtifact] = field(default_factory=list)
    analysis_steps: List[Dict[str, Any]] = field(default_factory=list)
    final_report: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)

class OllamaLLMClient:
    """Enhanced Ollama client with artifact tracking."""

    def __init__(self, base_url: str = OLLAMA_BASE_URL, model: str = OLLAMA_MODEL):
        self.base_url = base_url
        self.model = model
        self.client = httpx.AsyncClient(timeout=120.0)
        self.requests_made = 0
        self.total_tokens = 0

    async def generate(self, prompt: str, context: str = "", max_tokens: int = 500,
                      prompt_artifact: PromptArtifact = None) -> str:
        """Generate response from Ollama with tracking."""
        self.requests_made += 1

        try:
            full_prompt = f"{context}\n\n{prompt}" if context else prompt

            start_time = time.time()
            response = await self.client.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "num_predict": max_tokens,
                        "temperature": 0.3,
                    }
                }
            )
            response_time = time.time() - start_time

            if response.status_code == 200:
                result = response.json()
                response_text = result.get("response", "").strip()

                # Track performance
                if prompt_artifact:
                    prompt_artifact.usage_count += 1
                    prompt_artifact.performance_metrics = {
                        "total_requests": prompt_artifact.usage_count,
                        "avg_response_time": response_time,
                        "last_used": datetime.now(timezone.utc).isoformat()
                    }

                return response_text
            else:
                error_msg = f"Ollama API error: {response.status_code}"
                print(error_msg)
                return error_msg

        except Exception as e:
            error_msg = f"Ollama connection error: {e}"
            print(error_msg)
            return error_msg

class EcosystemTestHarness:
    """Comprehensive test harness for the entire ecosystem."""

    def __init__(self):
        self.ollama_client = OllamaLLMClient()
        self.workflow = AnalysisWorkflow(workflow_id=str(uuid.uuid4()))
        self.document_store: Dict[str, DocumentArtifact] = {}
        self.prompt_store: Dict[str, PromptArtifact] = {}

        # Initialize test data
        self._initialize_test_data()

    def _initialize_test_data(self):
        """Initialize comprehensive test data."""
        print("üìã Initializing test data...")

        # Create PR document
        pr_doc = DocumentArtifact(
            id="pr-oauth2-auth-001",
            title="Implement OAuth2 Authentication Service",
            content="""
            # Pull Request: Implement OAuth2 Authentication Service

            ## Summary
            This PR implements OAuth2 authentication for the user service with the following key changes:
            - Add OAuth2 client configuration in `src/auth/oauth2_client.py`
            - Implement token validation middleware in `src/auth/middleware.py`
            - Add user authentication endpoints in `src/api/auth_endpoints.py`
            - Update API documentation in `docs/api/authentication.md`

            ## Files Changed
            - `src/auth/oauth2_client.py` (+150 lines)
            - `src/auth/middleware.py` (+89 lines)
            - `src/api/auth_endpoints.py` (+67 lines)
            - `docs/api/authentication.md` (+45 lines)

            ## Related Issues
            - JIRA: PROJ-456 (OAuth2 Authentication Implementation)
            - Confluence: API_AUTH_DOCS, SECURITY_GUIDE

            ## Testing
            - Unit tests added for OAuth2 client
            - Integration tests for middleware
            - API endpoint tests

            ## Security Considerations
            - JWT token validation
            - Secure token storage
            - Rate limiting for auth endpoints
            """,
            source="github",
            type="pr",
            metadata={"author": "developer@example.com", "branch": "feature/oauth2-auth"}
        )

        # Create Jira ticket
        jira_doc = DocumentArtifact(
            id="jira-proj-456",
            title="Implement OAuth2 Authentication",
            content="""
            # JIRA Ticket: PROJ-456

            ## Issue Summary
            As a user, I want to authenticate using OAuth2 so that I can securely access the API.

            ## Description
            The current authentication system uses basic username/password which is not secure enough
            for production use. We need to implement OAuth2 authentication to provide better security
            and user experience.

            ## Acceptance Criteria
            1. User can authenticate with OAuth2 provider (Google, GitHub, etc.)
            2. API validates OAuth2 tokens on all protected endpoints
            3. Token refresh mechanism implemented
            4. Documentation updated with new authentication flow
            5. Security audit passed

            ## Technical Requirements
            - OAuth2 client library integration
            - JWT token validation middleware
            - Token refresh endpoint
            - Database schema for user sessions
            - API documentation updates

            ## Story Points: 8
            ## Priority: High
            ## Assignee: developer@example.com
            """,
            source="jira",
            type="jira",
            metadata={"priority": "High", "story_points": 8, "assignee": "developer@example.com"}
        )

        # Create Confluence documentation
        confluence_doc = DocumentArtifact(
            id="confluence-api-auth-docs",
            title="Authentication API Documentation",
            content="""
            # Authentication Service API Documentation

            ## Overview
            The Authentication Service provides secure user authentication and authorization
            for all API endpoints using industry-standard protocols.

            ## OAuth2 Flow Implementation

            ### 1. Authorization Request
            ```
            GET /auth/authorize?client_id=123&redirect_uri=https://app.example.com/callback&scope=read+write
            ```

            ### 2. User Authentication
            User is redirected to OAuth2 provider login page.

            ### 3. Authorization Code
            Provider redirects back with authorization code.

            ### 4. Token Exchange
            ```
            POST /auth/token
            Content-Type: application/x-www-form-urlencoded

            grant_type=authorization_code&code=abc123&redirect_uri=https://app.example.com/callback
            ```

            ### 5. API Access
            Use access token in Authorization header:
            ```
            Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
            ```

            ## Endpoints

            ### POST /auth/login
            Initiate OAuth2 login flow.

            ### POST /auth/token
            Exchange authorization code for access token.

            ### POST /auth/refresh
            Refresh expired access token.

            ### GET /auth/user
            Get authenticated user information.

            ### POST /auth/logout
            Invalidate user session.

            ## Security Requirements

            ### Token Validation
            - All protected endpoints must validate JWT tokens
            - Tokens must include user ID, roles, and expiration
            - Invalid tokens must return 401 Unauthorized

            ### Token Expiration
            - Access tokens: 1 hour
            - Refresh tokens: 30 days
            - Expired tokens must be rejected

            ### Rate Limiting
            - Auth endpoints: 10 requests per minute per IP
            - Token refresh: 5 requests per minute per user

            ## Error Handling

            ### Common Error Codes
            - `invalid_grant`: Invalid authorization code
            - `invalid_token`: Invalid or expired token
            - `insufficient_scope`: Token lacks required permissions
            - `access_denied`: User denied authorization

            ## Best Practices
            1. Always use HTTPS
            2. Validate all tokens on every request
            3. Implement proper error handling
            4. Log authentication events
            5. Monitor for suspicious activity
            """,
            source="confluence",
            type="confluence",
            metadata={"space": "API Documentation", "last_updated": "2024-09-10T14:30:00Z"}
        )

        # Create Security Guide document
        security_doc = DocumentArtifact(
            id="confluence-security-guide",
            title="Security Implementation Guide",
            content="""
            # Security Implementation Guide

            ## Authentication Security

            ### OAuth2 Implementation Requirements
            1. **Secure Token Storage**
               - Never store tokens in local storage
               - Use HttpOnly cookies for refresh tokens
               - Implement token rotation

            2. **Token Validation**
               - Validate JWT signature on every request
               - Check token expiration
               - Verify token issuer and audience
               - Implement token blacklisting for logout

            3. **Authorization**
               - Implement role-based access control (RBAC)
               - Use scopes for fine-grained permissions
               - Validate user permissions on every request

            ### Security Headers
            - `Strict-Transport-Security`: max-age=31536000
            - `X-Content-Type-Options`: nosniff
            - `X-Frame-Options`: DENY
            - `Content-Security-Policy`: default-src 'self'

            ## Common Security Vulnerabilities

            ### 1. Token Theft
            - **Prevention**: Use secure HTTPS only
            - **Detection**: Monitor for unusual token usage
            - **Response**: Immediate token invalidation

            ### 2. Session Hijacking
            - **Prevention**: Implement session timeouts
            - **Detection**: Monitor IP address changes
            - **Response**: Force re-authentication

            ### 3. Brute Force Attacks
            - **Prevention**: Implement rate limiting
            - **Detection**: Monitor failed authentication attempts
            - **Response**: Temporary account lockout

            ## Security Audit Checklist

            ### Authentication
            - [ ] OAuth2 flow implemented correctly
            - [ ] Tokens validated on every request
            - [ ] Secure token storage implemented
            - [ ] Rate limiting configured
            - [ ] Security headers set

            ### Authorization
            - [ ] RBAC implemented
            - [ ] Permissions validated
            - [ ] Admin role restrictions
            - [ ] API key rotation policy

            ### Monitoring
            - [ ] Authentication events logged
            - [ ] Security alerts configured
            - [ ] Audit trail maintained
            """,
            source="confluence",
            type="confluence",
            metadata={"space": "Security", "last_updated": "2024-09-08T10:15:00Z"}
        )

        # Store documents
        self.document_store.update({
            pr_doc.id: pr_doc,
            jira_doc.id: jira_doc,
            confluence_doc.id: confluence_doc,
            security_doc.id: security_doc
        })

        print(f"‚úÖ Created {len(self.document_store)} test documents")

        # Create analysis prompts
        self._initialize_analysis_prompts()

    def _initialize_analysis_prompts(self):
        """Initialize analysis prompts with versioning."""
        print("üìù Initializing analysis prompts...")

        # Requirements alignment prompt
        req_alignment_prompt = PromptArtifact(
            id="prompt-req-alignment-v2",
            name="Requirements Alignment Analysis",
            content="""
You are an expert software engineer analyzing a GitHub Pull Request against its requirements from Jira and Confluence documentation.

## Analysis Task
Compare the PR implementation against the requirements and provide a detailed assessment.

## PR Details
{pr_content}

## Requirements (Jira)
{jira_content}

## Documentation (Confluence)
{confluence_content}

## Analysis Requirements
1. **Overall Alignment Score**: Rate how well the PR implements the requirements (0-10)
2. **Requirements Coverage**:
   - Which requirements are fully implemented
   - Which are partially implemented
   - Which are missing or not addressed
3. **Technical Implementation Quality**:
   - Code quality and architecture
   - Security considerations
   - Performance implications
4. **Documentation Consistency**:
   - Alignment with existing API documentation
   - Updates to documentation required
5. **Gaps and Issues**:
   - Missing functionality
   - Potential problems or risks
   - Inconsistencies with requirements

## Output Format
Provide a structured analysis with specific findings, scores, and actionable recommendations.
Be technical and specific in your assessment.
""",
            category="requirements_analysis",
            version="2.0"
        )

        # Documentation consistency prompt
        doc_consistency_prompt = PromptArtifact(
            id="prompt-doc-consistency-v1",
            name="Documentation Consistency Analysis",
            content="""
You are analyzing the consistency between code changes and existing documentation.

## Analysis Focus
Evaluate how well the PR implementation aligns with existing documentation and identify any inconsistencies.

## PR Changes
{pr_content}

## Existing Documentation
{confluence_content}

## Security Guidelines
{security_content}

## Analysis Areas
1. **API Consistency**:
   - Do code changes match documented APIs?
   - Are there breaking changes not reflected in docs?
   - New endpoints properly documented?

2. **Security Compliance**:
   - Implementation follows security guidelines?
   - Security requirements properly addressed?
   - Potential security vulnerabilities introduced?

3. **Documentation Updates Needed**:
   - What documentation needs updating?
   - Missing API documentation?
   - Outdated examples or guides?

4. **Technical Accuracy**:
   - Code implementation matches documented behavior?
   - Configuration changes documented?
   - Deployment procedures updated?

## Output Requirements
Provide specific findings about consistency issues or confirm areas of good alignment.
Identify any risks or gaps in documentation coverage.
""",
            category="consistency_check",
            version="1.0"
        )

        # Confidence scoring prompt
        confidence_prompt = PromptArtifact(
            id="prompt-confidence-scoring-v3",
            name="PR Confidence Assessment",
            content="""
You are providing a comprehensive confidence assessment for a pull request approval decision.

## Analysis Context
Based on detailed analysis of requirements alignment and documentation consistency, provide an overall confidence assessment.

## Analysis Results
Requirements Alignment Score: {req_score}/10
Documentation Consistency Score: {doc_score}/10
Identified Gaps: {gap_count}
Identified Risks: {risk_count}

## Requirements Analysis Summary
{req_analysis}

## Documentation Analysis Summary
{doc_analysis}

## Assessment Framework
Provide:
1. **Overall Confidence Score** (0-100): Based on all analysis factors
2. **Confidence Level**: High/Medium/Low/Critical
3. **Approval Recommendation**: Approve/Review_Required/Hold/Reject
4. **Key Rationale**: Explain your assessment with specific factors
5. **Critical Concerns**: Any blocking issues or major risks
6. **Recommendations**: Specific actions to improve confidence

## Decision Criteria
- **High Confidence (80-100)**: Requirements fully met, no critical issues, good documentation alignment
- **Medium Confidence (60-79)**: Most requirements met, minor issues, documentation mostly aligned
- **Low Confidence (40-59)**: Significant gaps, documentation inconsistencies, needs review
- **Critical (<40)**: Major requirements missing, security issues, documentation completely out of sync

Consider enterprise software standards, security requirements, and production readiness in your assessment.
""",
            category="confidence_scoring",
            version="3.0"
        )

        # Store prompts
        self.prompt_store.update({
            req_alignment_prompt.id: req_alignment_prompt,
            doc_consistency_prompt.id: doc_consistency_prompt,
            confidence_prompt.id: confidence_prompt
        })

        print(f"‚úÖ Created {len(self.prompt_store)} analysis prompts")

    async def run_comprehensive_analysis(self):
        """Run the complete PR confidence analysis workflow."""
        print("\\nüöÄ STARTING COMPREHENSIVE PR CONFIDENCE ANALYSIS")
        print("=" * 70)

        # Step 1: Test Ollama connectivity
        print("\\n1. üîó Testing Ollama LLM Integration...")
        connectivity_test = await self.ollama_client.generate("Hello! Confirm you are ready for analysis.")
        if "Error" not in connectivity_test:
            print("‚úÖ Ollama LLM connected and ready")
            print(f"   Model: {OLLAMA_MODEL}")
            print(f"   Response: {connectivity_test}")
        else:
            print(f"‚ùå Ollama connection failed: {connectivity_test}")
            return None

        # Step 2: Document Analysis Preparation
        print("\\n2. üìÑ Preparing Document Analysis...")

        # Get documents
        pr_doc = self.document_store["pr-oauth2-auth-001"]
        jira_doc = self.document_store["jira-proj-456"]
        confluence_doc = self.document_store["confluence-api-auth-docs"]
        security_doc = self.document_store["confluence-security-guide"]

        # Establish document relationships
        pr_doc.related_documents = [jira_doc.id, confluence_doc.id, security_doc.id]
        jira_doc.related_documents = [pr_doc.id, confluence_doc.id]
        confluence_doc.related_documents = [pr_doc.id, jira_doc.id, security_doc.id]
        security_doc.related_documents = [pr_doc.id, confluence_doc.id]

        print(f"‚úÖ Prepared {len(self.document_store)} documents for analysis")
        print(f"   PR: {pr_doc.title}")
        print(f"   Jira: {jira_doc.title}")
        print(f"   Confluence Docs: {confluence_doc.title}, {security_doc.title}")

        # Step 3: Requirements Alignment Analysis
        print("\\n3. üîç Running Requirements Alignment Analysis...")

        req_prompt = self.prompt_store["prompt-req-alignment-v2"]
        req_analysis_start = time.time()

        req_prompt_content = req_prompt.content.format(
            pr_content=pr_doc.content,
            jira_content=jira_doc.content,
            confluence_content=confluence_doc.content
        )

        req_analysis_response = await self.ollama_client.generate(
            req_prompt_content,
            "You are analyzing software requirements alignment between a pull request and its specifications.",
            max_tokens=1200,
            prompt_artifact=req_prompt
        )

        req_analysis_time = time.time() - req_analysis_start

        # Create analysis result document
        req_analysis_doc = DocumentArtifact(
            id=f"analysis-req-alignment-{uuid.uuid4().hex[:8]}",
            title="Requirements Alignment Analysis Result",
            content=req_analysis_response,
            source="llm_analysis",
            type="analysis_result",
            related_documents=[pr_doc.id, jira_doc.id, confluence_doc.id],
            metadata={
                "analysis_type": "requirements_alignment",
                "prompt_used": req_prompt.id,
                "analysis_time": req_analysis_time,
                "llm_model": OLLAMA_MODEL
            }
        )

        self.document_store[req_analysis_doc.id] = req_analysis_doc
        self.workflow.analysis_steps.append({
            "step": "requirements_alignment",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "prompt_id": req_prompt.id,
            "documents_used": [pr_doc.id, jira_doc.id, confluence_doc.id],
            "analysis_time": req_analysis_time,
            "result_document": req_analysis_doc.id
        })

        print("‚úÖ Requirements alignment analysis completed")
        print(f"   Analysis Time: {req_analysis_time:.2f}s")
        print(f"   Prompt Used: {req_prompt.name} (v{req_prompt.version})")
        print(f"   Result Document: {req_analysis_doc.id}")

        # Step 4: Documentation Consistency Analysis
        print("\\n4. üìö Running Documentation Consistency Analysis...")

        doc_prompt = self.prompt_store["prompt-doc-consistency-v1"]
        doc_analysis_start = time.time()

        doc_prompt_content = doc_prompt.content.format(
            pr_content=pr_doc.content,
            confluence_content=confluence_doc.content,
            security_content=security_doc.content
        )

        doc_analysis_response = await self.ollama_client.generate(
            doc_prompt_content,
            "You are evaluating documentation consistency in software development.",
            max_tokens=1000,
            prompt_artifact=doc_prompt
        )

        doc_analysis_time = time.time() - doc_analysis_start

        # Create analysis result document
        doc_analysis_doc = DocumentArtifact(
            id=f"analysis-doc-consistency-{uuid.uuid4().hex[:8]}",
            title="Documentation Consistency Analysis Result",
            content=doc_analysis_response,
            source="llm_analysis",
            type="analysis_result",
            related_documents=[pr_doc.id, confluence_doc.id, security_doc.id],
            metadata={
                "analysis_type": "documentation_consistency",
                "prompt_used": doc_prompt.id,
                "analysis_time": doc_analysis_time,
                "llm_model": OLLAMA_MODEL
            }
        )

        self.document_store[doc_analysis_doc.id] = doc_analysis_doc
        self.workflow.analysis_steps.append({
            "step": "documentation_consistency",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "prompt_id": doc_prompt.id,
            "documents_used": [pr_doc.id, confluence_doc.id, security_doc.id],
            "analysis_time": doc_analysis_time,
            "result_document": doc_analysis_doc.id
        })

        print("‚úÖ Documentation consistency analysis completed")
        print(f"   Analysis Time: {doc_analysis_time:.2f}s")
        print(f"   Prompt Used: {doc_prompt.name} (v{doc_prompt.version})")
        print(f"   Result Document: {doc_analysis_doc.id}")

        # Step 5: Confidence Assessment
        print("\\n5. üéØ Generating Overall Confidence Assessment...")

        confidence_prompt = self.prompt_store["prompt-confidence-scoring-v3"]
        confidence_start = time.time()

        # Extract scores from previous analyses (simplified for demo)
        req_score = 8.5  # Extract from req_analysis_response
        doc_score = 7.2  # Extract from doc_analysis_response
        gap_count = 3
        risk_count = 2

        confidence_prompt_content = confidence_prompt.content.format(
            req_score=req_score,
            doc_score=doc_score,
            gap_count=gap_count,
            risk_count=risk_count,
            req_analysis=req_analysis_response[:500] + "...",
            doc_analysis=doc_analysis_response[:500] + "..."
        )

        confidence_response = await self.ollama_client.generate(
            confidence_prompt_content,
            "You are providing a final approval recommendation for a software pull request.",
            max_tokens=800,
            prompt_artifact=confidence_prompt
        )

        confidence_time = time.time() - confidence_start

        # Extract confidence score and level from response
        confidence_score = 75  # Default
        confidence_level = "medium"
        approval_rec = "review_required"

        import re
        score_match = re.search(r'(\d+)', confidence_response[:200])
        if score_match:
            confidence_score = int(score_match.group(1))
            if confidence_score >= 80:
                confidence_level = "high"
                approval_rec = "approve"
            elif confidence_score >= 60:
                confidence_level = "medium"
                approval_rec = "review_required"
            else:
                confidence_level = "low"
                approval_rec = "hold"

        # Create final report document
        final_report_doc = DocumentArtifact(
            id=f"report-pr-confidence-{uuid.uuid4().hex[:8]}",
            title="PR Confidence Analysis Report",
            content=f"""
# PR Confidence Analysis Report

## Executive Summary
**Confidence Score: {confidence_score}% ({confidence_level.upper()})**
**Recommendation: {approval_rec.replace('_', ' ').title()}**

## Analysis Overview
- **PR**: {pr_doc.title}
- **Jira Ticket**: {jira_doc.title}
- **Analysis Date**: {datetime.now(timezone.utc).isoformat()}

## Detailed Findings

### Requirements Alignment ({req_score}/10)
{req_analysis_response[:300]}...

### Documentation Consistency ({doc_score}/10)
{doc_analysis_response[:300]}...

### Overall Assessment
{confidence_response[:400]}...

## Analysis Metadata
- **Analysis Time**: {self.workflow.end_time - self.workflow.start_time if self.workflow.end_time else 'N/A'}
- **LLM Model**: {OLLAMA_MODEL}
- **Prompts Used**: {len(self.prompt_store)}
- **Documents Analyzed**: {len(self.document_store)}
""",
            source="llm_analysis",
            type="report",
            related_documents=[pr_doc.id, jira_doc.id, confluence_doc.id, security_doc.id,
                             req_analysis_doc.id, doc_analysis_doc.id],
            metadata={
                "report_type": "pr_confidence_analysis",
                "confidence_score": confidence_score,
                "confidence_level": confidence_level,
                "approval_recommendation": approval_rec,
                "analysis_time": confidence_time,
                "llm_model": OLLAMA_MODEL,
                "prompts_used": list(self.prompt_store.keys()),
                "documents_used": list(self.document_store.keys())
            }
        )

        self.document_store[final_report_doc.id] = final_report_doc
        self.workflow.analysis_steps.append({
            "step": "confidence_assessment",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "prompt_id": confidence_prompt.id,
            "documents_used": [req_analysis_doc.id, doc_analysis_doc.id],
            "analysis_time": confidence_time,
            "result_document": final_report_doc.id
        })

        print("‚úÖ Confidence assessment completed")
        print(f"   Analysis Time: {confidence_time:.2f}s")
        print(f"   Confidence Score: {confidence_score}%")
        print(f"   Level: {confidence_level.upper()}")
        print(f"   Recommendation: {approval_rec.replace('_', ' ').title()}")

        # Step 6: Complete workflow
        self.workflow.end_time = datetime.now(timezone.utc)
        total_time = (self.workflow.end_time - self.workflow.start_time).total_seconds()

        self.workflow.performance_metrics = {
            "total_analysis_time": total_time,
            "ollama_requests": self.ollama_client.requests_made,
            "documents_created": len(self.document_store),
            "prompts_used": len(self.prompt_store),
            "steps_completed": len(self.workflow.analysis_steps)
        }

        self.workflow.documents_used = list(self.document_store.values())
        self.workflow.prompts_used = list(self.prompt_store.values())
        self.workflow.final_report = {
            "report_id": final_report_doc.id,
            "confidence_score": confidence_score,
            "confidence_level": confidence_level,
            "approval_recommendation": approval_rec,
            "summary": confidence_response[:200] + "..."
        }

        print("\\nüéâ COMPREHENSIVE ANALYSIS COMPLETED!")
        print("=" * 50)

        return self._generate_comprehensive_report()

    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate comprehensive report with all artifacts and relationships."""
        print("\\nüìä Generating Comprehensive Report...")

        # Build document relationships graph
        document_graph = {}
        for doc_id, doc in self.document_store.items():
            document_graph[doc_id] = {
                "title": doc.title,
                "type": doc.type,
                "source": doc.source,
                "related_documents": doc.related_documents,
                "metadata": doc.metadata,
                "created_at": doc.created_at.isoformat()
            }

        # Build prompt usage summary
        prompt_summary = {}
        for prompt_id, prompt in self.prompt_store.items():
            prompt_summary[prompt_id] = {
                "name": prompt.name,
                "category": prompt.category,
                "version": prompt.version,
                "usage_count": prompt.usage_count,
                "performance_metrics": prompt.performance_metrics,
                "related_analyses": prompt.related_analyses,
                "created_at": prompt.created_at.isoformat()
            }

        # Build analysis workflow summary
        workflow_summary = {
            "workflow_id": self.workflow.workflow_id,
            "start_time": self.workflow.start_time.isoformat(),
            "end_time": self.workflow.end_time.isoformat() if self.workflow.end_time else None,
            "total_duration": (self.workflow.end_time - self.workflow.start_time).total_seconds() if self.workflow.end_time else None,
            "steps_completed": len(self.workflow.analysis_steps),
            "performance_metrics": self.workflow.performance_metrics
        }

        # Build final comprehensive report
        comprehensive_report = {
            "report_metadata": {
                "generated_at": datetime.now(timezone.utc).isoformat(),
                "report_type": "comprehensive_pr_confidence_analysis",
                "llm_model": OLLAMA_MODEL,
                "analysis_framework": "Real LLM-powered with Ollama"
            },
            "executive_summary": {
                "confidence_score": self.workflow.final_report.get("confidence_score", 0),
                "confidence_level": self.workflow.final_report.get("confidence_level", "unknown"),
                "approval_recommendation": self.workflow.final_report.get("approval_recommendation", "unknown"),
                "total_analysis_time": self.workflow.performance_metrics.get("total_analysis_time", 0),
                "documents_analyzed": len(self.document_store),
                "prompts_used": len(self.prompt_store)
            },
            "document_ecosystem": {
                "total_documents": len(self.document_store),
                "document_types": {
                    doc_type: len([d for d in self.document_store.values() if d.type == doc_type])
                    for doc_type in set(d.type for d in self.document_store.values())
                },
                "document_sources": {
                    source: len([d for d in self.document_store.values() if d.source == source])
                    for source in set(d.source for d in self.document_store.values())
                },
                "document_relationships": document_graph
            },
            "prompt_ecosystem": {
                "total_prompts": len(self.prompt_store),
                "prompt_categories": {
                    category: len([p for p in self.prompt_store.values() if p.category == category])
                    for category in set(p.category for p in self.prompt_store.values())
                },
                "prompt_performance": prompt_summary
            },
            "analysis_workflow": {
                "workflow_summary": workflow_summary,
                "analysis_steps": self.workflow.analysis_steps,
                "step_details": {
                    step["step"]: {
                        "duration": step["analysis_time"],
                        "documents_used": len(step["documents_used"]),
                        "prompt_used": step["prompt_id"],
                        "result_document": step["result_document"]
                    } for step in self.workflow.analysis_steps
                }
            },
            "llm_integration": {
                "model_used": OLLAMA_MODEL,
                "total_requests": self.ollama_client.requests_made,
                "requests_per_step": {
                    step["step"]: 1 for step in self.workflow.analysis_steps
                },
                "average_response_time": sum(
                    step["analysis_time"] for step in self.workflow.analysis_steps
                ) / len(self.workflow.analysis_steps) if self.workflow.analysis_steps else 0
            },
            "detailed_documents": {
                doc_id: {
                    "id": doc.id,
                    "title": doc.title,
                    "type": doc.type,
                    "source": doc.source,
                    "content_preview": doc.content[:200] + "..." if len(doc.content) > 200 else doc.content,
                    "related_documents": doc.related_documents,
                    "metadata": doc.metadata,
                    "version": doc.version,
                    "created_at": doc.created_at.isoformat()
                } for doc_id, doc in self.document_store.items()
            },
            "detailed_prompts": {
                prompt_id: {
                    "id": prompt.id,
                    "name": prompt.name,
                    "category": prompt.category,
                    "content": prompt.content,
                    "version": prompt.version,
                    "usage_count": prompt.usage_count,
                    "performance_metrics": prompt.performance_metrics,
                    "related_analyses": prompt.related_analyses,
                    "created_at": prompt.created_at.isoformat()
                } for prompt_id, prompt in self.prompt_store.items()
            },
            "analysis_results": {
                "final_report_id": self.workflow.final_report.get("report_id"),
                "confidence_assessment": self.workflow.final_report,
                "analysis_artifacts": [
                    {
                        "document_id": doc.id,
                        "type": doc.type,
                        "title": doc.title,
                        "analysis_type": doc.metadata.get("analysis_type"),
                        "prompt_used": doc.metadata.get("prompt_used"),
                        "analysis_time": doc.metadata.get("analysis_time")
                    } for doc in self.document_store.values()
                    if doc.type in ["analysis_result", "report"]
                ]
            },
            "data_lineage": {
                "document_flow": {
                    "source_documents": [doc_id for doc_id, doc in self.document_store.items() if doc.type in ["pr", "jira", "confluence"]],
                    "analysis_documents": [doc_id for doc_id, doc in self.document_store.items() if doc.type == "analysis_result"],
                    "report_documents": [doc_id for doc_id, doc in self.document_store.items() if doc.type == "report"]
                },
                "prompt_lineage": {
                    category: [prompt_id for prompt_id, prompt in self.prompt_store.items() if prompt.category == category]
                    for category in set(p.category for p in self.prompt_store.values())
                },
                "workflow_dependencies": {
                    step["step"]: {
                        "depends_on": step["documents_used"],
                        "produces": step["result_document"],
                        "uses_prompt": step["prompt_id"]
                    } for step in self.workflow.analysis_steps
                }
            }
        }

        print("‚úÖ Comprehensive report generated")
        return comprehensive_report


async def main():
    """Main test execution function."""
    print("üß™ COMPREHENSIVE PR CONFIDENCE ANALYSIS TEST")
    print("Integrating Document Store, Prompt Store, Analysis Service, and LLM Gateway")
    print("=" * 80)

    # Initialize test harness
    harness = EcosystemTestHarness()

    # Run comprehensive analysis
    start_time = time.time()
    results = await harness.run_comprehensive_analysis()
    total_execution_time = time.time() - start_time

    if results:
        # Save comprehensive results
        output_file = "comprehensive_pr_analysis_results.json"
        with open(output_file, "w") as f:
            json.dump(results, f, indent=2, default=str)

        print(f"\\nüíæ Detailed results saved to: {output_file}")

        # Print summary
        print("\\nüéØ ANALYSIS SUMMARY:")
        print("-" * 30)
        print(f"   Confidence Score: {results['executive_summary']['confidence_score']}%")
        print(f"   Confidence Level: {results['executive_summary']['confidence_level'].upper()}")
        print(f"   Recommendation: {results['executive_summary']['approval_recommendation'].replace('_', ' ').title()}")
        print(f"   Total Analysis Time: {total_execution_time:.2f}s")
        print(f"   Documents Analyzed: {results['executive_summary']['documents_analyzed']}")
        print(f"   Prompts Used: {results['executive_summary']['prompts_used']}")
        print(f"   LLM Requests Made: {results['llm_integration']['total_requests']}")

        print("\\nüìä ECOSYSTEM INTEGRATION:")
        print("-" * 30)
        print(f"   Document Store: {results['document_ecosystem']['total_documents']} documents")
        print(f"   Prompt Store: {results['prompt_ecosystem']['total_prompts']} prompts")
        print(f"   Analysis Steps: {results['analysis_workflow']['workflow_summary']['steps_completed']}")
        print(f"   LLM Model: {OLLAMA_MODEL}")

        print("\\nüîó KEY RELATIONSHIPS:")
        print("-" * 30)
        for doc_type, count in results['document_ecosystem']['document_types'].items():
            print(f"   {doc_type.title()}: {count} documents")

        print("\\n‚úÖ SUCCESS: Complete ecosystem integration test passed!")
        print("   Real LLM analysis with full artifact tracking and relationship mapping!")

        return results
    else:
        print("\\n‚ùå FAILED: Comprehensive analysis did not complete successfully.")
        return None


if __name__ == "__main__":
    asyncio.run(main())
