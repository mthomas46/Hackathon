version: "3.9"

services:
  # Redis - Required dependency
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # LLM Gateway - Standalone service
  llm-gateway:
    build:
      context: ../..
      dockerfile: services/llm-gateway/Dockerfile
    volumes:
      - llm_gateway_cache:/app/cache
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=redis
      - ENVIRONMENT=development
      # LLM Provider Configuration
      - OLLAMA_ENDPOINT=http://host.docker.internal:11434
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - BEDROCK_ENDPOINT=http://host.docker.internal:5060/invoke
      - GROK_API_KEY=${GROK_API_KEY:-}
      # Security Configuration
      - SENSITIVE_KEYWORDS=password,secret,token,key,credential,confidential,internal
      - SENSITIVITY_THRESHOLD=0.7
      - SECURE_ONLY_MODELS=ollama,bedrock
      - ALL_PROVIDERS=bedrock,ollama,openai,anthropic,grok
      # Caching Configuration
      - CACHE_MAX_SIZE=1000
      - CACHE_DEFAULT_TTL=3600
      - CACHE_CLEANUP_INTERVAL=300
      # Rate Limiting Configuration
      - RATE_LIMIT_REQUESTS_PER_MINUTE=60
      - RATE_LIMIT_REQUESTS_PER_HOUR=1000
      - RATE_LIMIT_TOKENS_PER_MINUTE=50000
      - RATE_LIMIT_BURST_LIMIT=10
      - RATE_LIMIT_COOLDOWN_SECONDS=60
    ports:
      - "5055:5055"
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5055/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  redis_data:
    driver: local
  llm_gateway_cache:
    driver: local

networks:
  default:
    name: llm-gateway-network
    driver: bridge
