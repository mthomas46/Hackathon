# =============================================================================
# DOCKER COMPOSE - DEVELOPMENT ENVIRONMENT
# =============================================================================
# Comprehensive development setup for the LLM Documentation Ecosystem
#
# Services included:
# - Core Infrastructure: Redis, PostgreSQL (optional)
# - Core Services: orchestrator, doc_store, analysis-service, source-agent, frontend
# - AI Services: summarizer-hub, bedrock-proxy, github-mcp
# - Agent Services: memory-agent, discovery-agent, notification-service
# - Utility Services: prompt-store, interpreter, cli
# - Analysis Services: code-analyzer, secure-analyzer, log-collector
#
# Use profiles to control which services start:
#   docker-compose --profile core up              # Core services only
#   docker-compose --profile development up       # Development services
#   docker-compose --profile ai_services up       # AI/ML services
#   docker-compose --profile tooling up           # CLI and tooling
#
# Individual Services:
# Each service can run independently with its own dependencies:
#   cd services/{service-name} && docker-compose up
#
# Configuration:
# - Unified config.yml at project root
# - Service-specific configs in services/*/config.yaml
# - Environment variables override defaults
# =============================================================================

version: "3.9"

services:
  # ============================================================================
  # INFRASTRUCTURE SERVICES
  # ============================================================================

  # Redis - In-memory data store and cache
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - core
      - development
      - ai_services
      - production

  # PostgreSQL - Production database (optional, uncomment to enable)
  # postgres:
  #   image: postgres:15-alpine
  #   environment:
  #     PYTHONPATH: /app
  #     POSTGRES_DB: doc_consistency
  #     POSTGRES_USER: doc_user
  #     POSTGRES_PASSWORD: doc_password
  #     POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data
  #     - ./infrastructure/postgres-init.sql:/docker-entrypoint-initdb.d/init.sql:ro
  #   ports:
  #     - "5432:5432"
  #   healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U doc_user -d doc_consistency"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #   profiles:
  #     - production

  # ============================================================================
  # CORE SERVICES
  # ============================================================================

  # Orchestrator - Central coordination service
  orchestrator:
    build:
      context: .
      dockerfile: services/orchestrator/Dockerfile
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=redis
      - ENVIRONMENT=development
    ports:
      - "5099:5099"
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5099/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - core
      - ai_services

  # Document Store - Document storage and retrieval
  doc_store:
    build:
      context: .
      dockerfile: services/doc_store/Dockerfile
    volumes:
      - doc_store_data:/app/data
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=redis
      - ENVIRONMENT=development
    ports:
      - "5087:5010"
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - core
      - ai_services

  # Analysis Service - Document analysis and consistency checking
  analysis-service:
    build:
      context: .
      dockerfile: services/analysis-service/Dockerfile
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=redis
      - ENVIRONMENT=development
      # DDD-specific environment variables
      - DDD_ARCHITECTURE=true
      - DDD_CONFIG_FILE=config/ddd_config.yaml
    ports:
      - "5080:5080"
    depends_on:
      redis:
        condition: service_healthy
      doc_store:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - core
      - ai_services

  # Source Agent - Data ingestion from various sources
  source-agent:
    build:
      context: .
      dockerfile: services/source-agent/Dockerfile
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=redis
      - ENVIRONMENT=development
    ports:
      - "5070:5070"
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5070/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - core

  # Frontend - Web interface
  frontend:
    build:
      context: .
      dockerfile: services/frontend/Dockerfile
    environment:
      - PYTHONPATH=/app
      - ENVIRONMENT=development
    ports:
      - "3000:3000"
    depends_on:
      orchestrator:
        condition: service_started
      analysis-service:
        condition: service_started
    profiles:
      - core

  # ============================================================================
  # AI/ML SERVICES
  # ============================================================================

  # Ollama - Local LLM inference engine
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    profiles:
      - ai_services
    restart: unless-stopped

  # Summarizer Hub - AI-powered summarization service
  summarizer-hub:
    build:
      context: .
      dockerfile: services/summarizer-hub/Dockerfile
    environment:
      - PYTHONPATH=/app
      - OLLAMA_HOST=http://host.docker.internal:11434
      - ENVIRONMENT=development
    ports:
      - "5160:5160"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5160/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - ai_services

  # Architecture Digitizer - Diagram normalization service
  architecture-digitizer:
    build:
      context: .
      dockerfile: services/architecture-digitizer/Dockerfile
    environment:
      - PYTHONPATH=/app
      - ENVIRONMENT=development
    ports:
      - "5105:5105"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5105/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - ai_services

  # Bedrock Proxy - AWS Bedrock API proxy
  bedrock-proxy:
    build:
      context: .
      dockerfile: services/bedrock-proxy/Dockerfile
    environment:
      - PYTHONPATH=/app
      - ENVIRONMENT=development
    ports:
      - "5060:5060"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5060/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - ai_services

  # LLM Gateway - Unified access to all LLM providers
  llm-gateway:
    build:
      context: .
      dockerfile: services/llm-gateway/Dockerfile
    volumes:
      - llm_gateway_cache:/app/cache
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=redis
      - ENVIRONMENT=development
      # LLM Provider Configuration
      - OLLAMA_ENDPOINT=http://ollama:11434
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - BEDROCK_ENDPOINT=http://bedrock-proxy:5060/invoke
      - GROK_API_KEY=${GROK_API_KEY:-}
      # Security Configuration
      - SENSITIVE_KEYWORDS=password,secret,token,key,credential,confidential,internal
      - SENSITIVITY_THRESHOLD=0.7
      - SECURE_ONLY_MODELS=ollama,bedrock
      - ALL_PROVIDERS=bedrock,ollama,openai,anthropic,grok
      # Caching Configuration
      - CACHE_MAX_SIZE=1000
      - CACHE_DEFAULT_TTL=3600
      - CACHE_CLEANUP_INTERVAL=300
      # Rate Limiting Configuration
      - RATE_LIMIT_REQUESTS_PER_MINUTE=60
      - RATE_LIMIT_REQUESTS_PER_HOUR=1000
      - RATE_LIMIT_TOKENS_PER_MINUTE=50000
      - RATE_LIMIT_BURST_LIMIT=10
      - RATE_LIMIT_COOLDOWN_SECONDS=60
    ports:
      - "5055:5055"
    depends_on:
      redis:
        condition: service_healthy
      bedrock-proxy:
        condition: service_started
      ollama:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5055/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - ai_services

  # Mock Data Generator - LLM-integrated mock data generation
  mock-data-generator:
    build:
      context: .
      dockerfile: services/mock-data-generator/Dockerfile
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=redis
      - ENVIRONMENT=development
      - LLM_GATEWAY_URL=http://llm-gateway:5055
      - DOC_STORE_URL=http://doc_store:5087
    ports:
      - "5065:5065"
    depends_on:
      llm-gateway:
        condition: service_healthy
      doc_store:
        condition: service_started
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5065/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - ai_services

  # GitHub MCP - GitHub Model Context Protocol service
  github-mcp:
    build:
      context: .
      dockerfile: services/github-mcp/Dockerfile
    environment:
      - PYTHONPATH=/app
      - ENVIRONMENT=development
    ports:
      - "5030:5030"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5030/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - ai_services

  # ============================================================================
  # AGENT SERVICES
  # ============================================================================

  # Memory Agent - Conversation memory and context management
  memory-agent:
    build:
      context: .
      dockerfile: services/memory-agent/Dockerfile
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=redis
      - ENVIRONMENT=development
    ports:
      - "5090:5090"
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - development

  # Discovery Agent - Service discovery and registration
  discovery-agent:
    build:
      context: .
      dockerfile: services/discovery-agent/Dockerfile
    environment:
      - PYTHONPATH=/app
      - ENVIRONMENT=development
    ports:
      - "5045:5045"
    volumes:
      - ./services/discovery-agent:/app/services/discovery_agent
      - ./services/shared:/app/services/shared
      - ./config.yml:/app/config.yml
      - ./config:/app/config
    depends_on:
      orchestrator:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5045/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - development

  # Notification Service - Notification delivery and management
  notification-service:
    build:
      context: .
      dockerfile: services/notification-service/Dockerfile
    environment:
      - PYTHONPATH=/app
      - ENVIRONMENT=development
    ports:
      - "5020:5020"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5020/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - production

  # ============================================================================
  # UTILITY SERVICES
  # ============================================================================

  # Prompt Store - Centralized prompt management
  prompt_store:
    build:
      context: .
      dockerfile: services/prompt_store/Dockerfile
    volumes:
      - prompt_store_data:/app/data
      - ./config.yml:/app/config.yml:ro
      - ./services/prompt_store/config.yaml:/app/services/prompt_store/config.yaml:ro
      - ./services/shared/config.yaml:/app/services/shared/config.yaml:ro
    environment:
      - PYTHONPATH=/app
      - REDIS_HOST=redis
      - ENVIRONMENT=development
    ports:
      - "5110:5110"
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5110/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - ai_services

  # Interpreter - Natural language processing and interpretation
  interpreter:
    build:
      context: .
      dockerfile: services/interpreter/Dockerfile
    volumes:
      - ./config.yml:/app/config.yml:ro
      - ./services/interpreter/config.yaml:/app/services/interpreter/config.yaml:ro
      - ./services/shared/config.yaml:/app/services/shared/config.yaml:ro
    environment:
      - PYTHONPATH=/app
      - ENVIRONMENT=development
    ports:
      - "5120:5120"
    depends_on:
      prompt_store:
        condition: service_started
      orchestrator:
        condition: service_started
      analysis-service:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5120/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - ai_services

  # CLI - Command line interface
  cli:
    build:
      context: .
      dockerfile: services/cli/Dockerfile
    volumes:
      - ./config.yml:/app/config.yml:ro
      - ./services/cli/config.yaml:/app/services/cli/config.yaml:ro
      - ./services/shared/config.yaml:/app/services/shared/config.yaml:ro
    environment:
      - PYTHONPATH=/app
      - ENVIRONMENT=development
    depends_on:
      prompt_store:
        condition: service_started
      orchestrator:
        condition: service_started
      analysis-service:
        condition: service_started
    command: ["sleep", "infinity"]  # Keep container running for interactive use
    profiles:
      - tooling

  # ============================================================================
  # ANALYSIS SERVICES
  # ============================================================================

  # Code Analyzer - Code analysis and security scanning
  code-analyzer:
    build:
      context: .
      dockerfile: services/code-analyzer/Dockerfile
    environment:
      - PYTHONPATH=/app
      - ENVIRONMENT=development
    ports:
      - "5050:5050"
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5050/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - production

  # Secure Analyzer - Content security and policy enforcement
  secure-analyzer:
    build:
      context: .
      dockerfile: services/secure-analyzer/Dockerfile
    environment:
      - PYTHONPATH=/app
      - ENVIRONMENT=development
    ports:
      - "5100:5100"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5100/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - production

  # Log Collector - Centralized logging service
  log-collector:
    build:
      context: .
      dockerfile: services/log-collector/Dockerfile
    environment:
      - PYTHONPATH=/app
      - ENVIRONMENT=development
    ports:
      - "5040:5040"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5040/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles:
      - production

# ============================================================================
# VOLUMES
# ============================================================================

volumes:
  # Redis persistence volume
  redis_data:
    driver: local

  # PostgreSQL data volume (uncomment if using PostgreSQL)
  # postgres_data:
  #   driver: local

  # Document store SQLite database volume
  doc_store_data:
    driver: local

  # Prompt store data volume
  prompt_store_data:
    driver: local

  # LLM Gateway cache volume
  llm_gateway_cache:
    driver: local

  # Ollama models and data volume
  ollama_data:
    driver: local

# ============================================================================
# NETWORKS
# ============================================================================

networks:
  # Main development network
  default:
    name: doc-ecosystem-dev
    driver: bridge


