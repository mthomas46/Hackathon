# Pytest configuration for LLM Documentation Ecosystem
# Optimized for performance and maintainability

[tool:pytest]
# Basic configuration
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Standardized markers for the LLM Documentation Ecosystem
markers =
    # Test Type Markers (Required - choose exactly one)
    unit: Fast, isolated unit tests that test individual functions or methods
    integration: Tests that verify interactions between multiple components or services
    performance: Tests that measure and validate system performance characteristics
    stress: Tests that validate system behavior under extreme load conditions
    security: Tests that validate security controls and vulnerability prevention

    # Service Markers (Optional - use when testing specific services)
    orchestrator: Tests for the orchestrator service (workflow management, service coordination)
    source_agent: Tests for the source agent service (data ingestion, source management)
    analysis_service: Tests for the analysis service (data analysis, reporting)
    doc_store: Tests for the document store service (document storage, retrieval)
    frontend: Tests for the frontend service (UI components, user interactions)
    memory_agent: Tests for the memory agent service (context management, memory operations)
    discovery_agent: Tests for the discovery agent service (service discovery, API exploration)
    interpreter: Tests for the interpreter service (query interpretation, natural language processing)
    cli: Tests for the CLI service (command-line interface, user commands)

    # Pattern Markers (Optional - describe testing approach)
    mocks: Tests that use mock objects for external dependencies
    fixture: Tests that rely heavily on custom fixtures
    parametrized: Tests that use parameterization for multiple input variations

    # Data Markers (Optional - classify by data requirements)
    data_heavy: Tests that require large amounts of test data
    realistic_data: Tests that use realistic production-like data
    edge_cases: Tests that focus on edge cases and boundary conditions

    # Condition Markers (Optional - special execution conditions)
    slow: Tests that take longer than average to execute (excluded in fast lanes)
    live: Tests that hit live/containerized services
    flaky: Tests that may be unstable and require special attention
    external: Tests that depend on external services or resources
    destructive: Tests that modify system state and require cleanup

    # Environment Markers (Optional - deployment environment specific)
    development: Tests specific to development environment
    staging: Tests specific to staging environment
    production: Tests specific to production environment

# Test execution configuration with marker integration
addopts =
    --strict-markers
    --strict-config
    --disable-warnings
    -ra
    --tb=short
    --cov=services
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-report=xml
    --marker-validation
    --marker-analytics
    -n auto
    --dist=loadfile

# Coverage configuration
[coverage:run]
source = services
omit =
    */tests/*
    */test_*
    */conftest.py
    */__pycache__/*
    */migrations/*
    */venv/*
    */env/*

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:

# Test data and fixtures
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning
    ignore::pytest.PytestUnknownMarkWarning
    error::Exception

# Performance test configuration
faulthandler_timeout = 300

# Logging configuration
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Parallel execution (optional - enable with -n auto)
# addopts = -n auto --dist loadfile

# Environment configuration
env =
    TESTING=true
    LOG_LEVEL=WARNING
    PYTHONDONTWRITEBYTECODE=1

# Cache configuration
cache_dir = .pytest_cache

# Result log configuration
junit_family = xunit2
junit_logging = all